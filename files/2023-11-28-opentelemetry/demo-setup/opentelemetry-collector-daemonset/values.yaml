mode: "daemonset"
fullnameOverride: otel-collector-daemonset

image:
  # See releases here (remove 'v' prefix): https://github.com/open-telemetry/opentelemetry-collector-contrib/releases
  tag: "0.89.0"

resources:
  requests:
    cpu: 70m
    memory: 384Mi

config:
  extensions:
    memory_ballast:
      size_mib: 150
      # size_in_percentage: 40

  receivers:
    filelog:
      # If downstream processing fails (i.e. memory_limiter says we're using too much memory)
      # this will keep reading from the same point in the logs instead of continuing.
      retry_on_failure:
        enabled: true
        max_elapsed_time: 5m # default '5m'. Setting to '0' means it will never stop retrying.
  processors:
    batch:
      send_batch_size: "8192"
      # send_batch_max_size >= send_batch_size.
      # This splits up batches that are too big for loki to handle (which can happen when
      # restarting the daemonset)
      send_batch_max_size: "8192"

    memory_limiter:
      check_interval: 5s
      # limit_percentage: 80
      # spike_limit_percentage: 25
      limit_mib: 384
      spike_limit_mib: 96

    attributes:
      actions:
        - action: insert
          key: loki.resource.labels
          value: namespace, app, deployment, statefulset, daemonset, cronjob

    k8sattributes:
      extract:
        metadata:
          - k8s.namespace.name
          - k8s.deployment.name
          - k8s.statefulset.name
          - k8s.daemonset.name
          - k8s.cronjob.name
          - k8s.job.name
          - k8s.node.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.container.name
          - container.id
          - container.image.name
          - container.image.tag
        labels:
          - { tag_name: team, key: team, from: pod }
          - { tag_name: app, key: app, from: pod }
          - { tag_name: node.type, key: "node.kubernetes.io/instance-type", from: node }
          - { tag_name: zone, key: "topology.kubernetes.io/zone", from: node }
          - { tag_name: node.provisioning, key: "cloud.google.com/gke-provisioning", from: node }
    resource:
      attributes:
        - { action: insert, key: service.version, from_attribute: container.image.tag }
        - { action: insert, key: namespace, from_attribute: k8s.namespace.name }
        - { action: insert, key: deployment, from_attribute: k8s.deployment.name }
        - { action: insert, key: container, from_attribute: k8s.container.name }
        - { action: insert, key: node, from_attribute: k8s.node.name }
        - { action: insert, key: statefulset, from_attribute: k8s.statefulset.name }
        - { action: insert, key: daemonset, from_attribute: k8s.daemonset.name }
        - { action: insert, key: cronjob, from_attribute: k8s.cronjob.name }
        - { action: insert, key: job, from_attribute: k8s.job.name }
        - { action: insert, key: pod, from_attribute: k8s.pod.name }
        - { action: insert, key: pod.ip, from_attribute: k8s.pod.ip }
        - { action: insert, key: uid, from_attribute: k8s.pod.uid }
        - { action: delete, pattern: ^k8s.* }

  exporters:
    loki:
      endpoint: <LOKI_ENDPOINT>
      headers:
        "X-Scope-OrgID": <TENANT>

  service:
    pipelines:
      # metrics and traces are instead collected by the opentelemetry-collector (a statefulset)
      metrics: null
      traces: null
      logs:
        receivers:
          - filelog
        processors:
          - memory_limiter
          - k8sattributes
          - resource
          - attributes
          - batch
        exporters: [loki]

presets:
  logsCollection:
    enabled: true
    includeCollectorLogs: true
    storeCheckpoints: true
  kubernetesAttributes:
    enabled: true

  hostMetrics:
    enabled: false
  kubernetesEvents:
    enabled: false
  clusterMetrics:
    enabled: false
  kubeletMetrics:
    enabled: false

tolerations:
  - operator: "Exists"

priorityClassName: <CHOOSE_PRIORITY>

prometheusRule:
  enabled: true
  defaultRules:
    enabled: true

podMonitor:
  enabled: true

ports:
  metrics:
    enabled: true
